import numpy as np
import tensorflow as tf
from sklearn.metrics import accuracy_score, classification_report
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Input
from tensorflow.keras.losses import binary_crossentropy


class Nnetwork:
    @staticmethod
    def evaluate_model(model, X_test, y_test):
        # 将标签 -1 转换为 0
        y_test = np.where(y_test == -1, 0, y_test)
        y_pred = (model.predict(X_test) > 0.5).astype(int)  # 将概率转换为类别

        accuracy = accuracy_score(y_test, y_pred)
        report = classification_report(y_test, y_pred)
        return accuracy, report

    @staticmethod
    def train_model(X_train, y_train, input_dim, hidden_units=64, epochs=5, learning_rate=0.01):
        # 构建模型
        model = Sequential([
            Input(shape=(input_dim,)),
            Dense(hidden_units, activation='relu'),
            Dense(hidden_units, activation='relu'),
            Dense(1, activation='sigmoid')
        ])

        # 编译模型
        optimizer = Adam(learning_rate=learning_rate)
        model.compile(optimizer=optimizer, loss=binary_crossentropy, metrics=['accuracy'])

        # 训练模型
        print("从头开始训练模型...")
        for epoch in range(epochs):
            with tf.GradientTape() as tape:
                logits = model(X_train, training=True)
                logits = tf.squeeze(logits, axis=-1)
                loss = tf.reduce_mean(binary_crossentropy(y_train, logits))
            grads = tape.gradient(loss, model.trainable_variables)
            optimizer.apply_gradients(zip(grads, model.trainable_variables))
            print(f"Training Epoch {epoch + 1}/{epochs} | Loss: {loss.numpy():.4f}")

        return model

    @staticmethod
    def predict_with_model(model, X):
        y_pred = (model.predict(X) > 0.5).astype(int)
        return y_pred
